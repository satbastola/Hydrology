{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec99960-92ba-4cf7-b9b2-c81f39b45b25",
   "metadata": {},
   "source": [
    "# üåä Flood Frequency Analysis Tool ‚Äì Overview & Documentation\n",
    "\n",
    "This tool reads peak flow data from the USGS NWIS database and fits 10 commonly used extreme value probability distributions to estimate flood magnitudes associated with various return periods (e.g., 2-year, 100-year). It performs statistical goodness-of-fit evaluation and provides an interactive interface to visualize the flood frequency curve for each distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß What the Tool Does\n",
    "\n",
    "- ‚úÖ Reads annual peak discharge data from a NWIS `.txt` file\n",
    "- ‚úÖ Fits multiple statistical distributions to the observed peak flows\n",
    "- ‚úÖ Computes estimated flood quantiles for specific return periods (2, 5, 10, 25, 50, 100 years)\n",
    "- ‚úÖ Calculates RMSE and Kolmogorov‚ÄìSmirnov (KS) goodness-of-fit metrics\n",
    "- ‚úÖ Allows the user to interactively select a distribution and view:\n",
    "  - Estimated peak flows\n",
    "  - Distribution parameters\n",
    "  - GOF statistics\n",
    "  - A flood frequency curve plotted in log scale\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ How to Use\n",
    "\n",
    "1. **Prepare Input File**  \n",
    "   - Download annual peak streamflow data from the [USGS NWIS Peak Flow site](https://waterdata.usgs.gov/nwis/peak)\n",
    "   - Save as a tab-delimited `.txt` file (e.g., `07022500_nwis_peak.txt`)\n",
    "\n",
    "2. **Run the Script in Jupyter Notebook**\n",
    "   - Place the file in your working directory\n",
    "   - Modify the line `usgs_file = \"07022500_nwis_peak.txt\"` to match your filename\n",
    "   - Run the script cell-by-cell\n",
    "\n",
    "3. **Explore Results**\n",
    "   - View the summary table of fitted distribution parameters and their statistical performance\n",
    "   - Use the dropdown selector to compare estimated flood flows and curves for each distribution\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Theoretical Background: Distributions Used\n",
    "\n",
    "Each distribution estimates the probability of rare flood events based on historical data. Here's a quick reference:\n",
    "\n",
    "| Distribution           | Description                                                                 | Parameters                        |\n",
    "|------------------------|-----------------------------------------------------------------------------|-----------------------------------|\n",
    "| **Gumbel (EV1)**        | Models block maxima (e.g., annual max). Skewed right.                      | Location (Œº), Scale (Œ≤)           |\n",
    "| **Log-Pearson III**     | Log-transformed Pearson Type III. Used in U.S. federal flood studies.      | Shape (Œ±), Location (Œº), Scale    |\n",
    "| **GEV**                 | General form for extremes. Includes Gumbel, Frechet, Weibull as cases.     | Shape (Œæ), Location, Scale        |\n",
    "| **Normal**              | Symmetric bell curve. May misrepresent skewed flood data.                  | Mean (Œº), Std. dev. (œÉ)           |\n",
    "| **Lognormal**           | Data is normally distributed after log transform. Skewed right.            | Shape (œÉ), Location, Scale        |\n",
    "| **Weibull (Type III)**  | Useful for extreme minimums or upper tails.                                | Shape (k), Location, Scale        |\n",
    "| **Exponential**         | Special case of Weibull; constant failure rate (rarely used for floods).   | Rate (Œª) or Scale                 |\n",
    "| **Gamma**               | General skewed distribution, flexible fit for hydrology                    | Shape (k), Scale (Œ∏), Location    |\n",
    "| **Loglogistic (Fisk)**  | Skewed right, like lognormal but heavier tail.                             | Shape (c), Location, Scale        |\n",
    "| **Generalized Pareto**  | Models excesses over a threshold (POT approach).                           | Shape, Location, Scale            |\n",
    "\n",
    "---\n",
    "\n",
    "## üìè Performance Evaluation Criteria\n",
    "\n",
    "Two statistical metrics assess how well each distribution fits the observed data:\n",
    "\n",
    "- ### üîπ Root Mean Squared Error (RMSE)\n",
    "  Measures average error between observed peak flows and estimated quantiles from the distribution:\n",
    "  $$\n",
    "  \\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum (Q_{\\text{obs}} - Q_{\\text{est}})^2 }\n",
    "  $$\n",
    "  Lower values indicate a better fit.\n",
    "\n",
    "- ### üîπ Kolmogorov‚ÄìSmirnov (KS) Statistic\n",
    "  Measures the maximum difference between the empirical cumulative distribution function (ECDF) and the theoretical CDF:\n",
    "  $$\n",
    "  D = \\sup_x |F_n(x) - F(x)|\n",
    "  $$\n",
    "  - Returns both the **KS statistic** and a **p-value**\n",
    "  - If p-value > 0.05: distribution is a statistically valid fit (‚úÖ Pass)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Output Summary\n",
    "\n",
    "- A sorted summary table of all distributions including:\n",
    "  - Fitted parameters\n",
    "  - RMSE\n",
    "  - KS statistic and p-value\n",
    "  - Pass/fail interpretation\n",
    "- Interactive flood frequency plots for return periods on a log-x axis\n",
    "- Ability to choose which distribution best represents the dataset\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Applications\n",
    "\n",
    "- Floodplain mapping\n",
    "- Hydraulic structure design (culverts, bridges, dams)\n",
    "- Return period‚Äìbased risk estimation\n",
    "- Hydrologic modeling calibration\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this tool extended with confidence intervals, percentile shading, or exported reports in Excel or PDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7643db02-d399-40cf-99b6-696997476e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satis\\AppData\\Local\\Temp\\ipykernel_108552\\3713513962.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['peak_dt'] = pd.to_datetime(df['peak_dt'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    site_no    peak_dt  peak_va\n",
      "2  07022500 1953-03-03    780.0\n",
      "3  07022500 1954-01-20    520.0\n",
      "4  07022500 1955-03-20    846.0\n",
      "5  07022500 1956-02-02    440.0\n",
      "6  07022500 1957-01-22    707.0\n",
      "\n",
      "üìä Goodness-of-Fit Summary for All Distributions:\n",
      "\n",
      "      Distribution            Parameters  RMSE (cfs)  KS Stat  KS p-value KS Result\n",
      "       Loglogistic   3.00, 70.55, 612.21       81.64    0.115       0.734    ‚úÖ Pass\n",
      "               GEV -0.18, 571.63, 277.22      109.57    0.121       0.671    ‚úÖ Pass\n",
      "           Weibull  0.86, 350.00, 439.66      115.93    0.160       0.334    ‚úÖ Pass\n",
      "         Lognormal   0.60, 71.09, 602.92      118.96    0.124       0.650    ‚úÖ Pass\n",
      "       Exponential        228.00, 564.61      133.56    0.171       0.258    ‚úÖ Pass\n",
      "   Log-Pearson III  1.56, 792.61, 456.75      139.16    0.117       0.710    ‚úÖ Pass\n",
      "             Gamma  1.64, 208.17, 356.96      139.16    0.117       0.710    ‚úÖ Pass\n",
      "Generalized Pareto -0.17, 219.25, 671.26      154.74    0.138       0.515    ‚úÖ Pass\n",
      "      Gumbel (EV1)        601.78, 303.40      179.93    0.093       0.910    ‚úÖ Pass\n",
      "            Normal        792.61, 484.53      235.87    0.193       0.151    ‚úÖ Pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826c528c21e843068c1d31863c348436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Distribution', options=('Gumbel (EV1)', 'Log-Pearson III', 'GEV', ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_selected_distribution(dist_name)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gumbel_r\n",
    "from ipywidgets import interact, Dropdown\n",
    "def read_nwis_peak_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a USGS NWIS peak flow .txt file and extracts peak flow data.\n",
    "    Handles comment lines and parses peak date and discharge values.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to NWIS peak flow text file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['site_no', 'peak_dt', 'peak_va'] (site, date, peak flow)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Identify the header (first non-commented line)\n",
    "        start_line = next(i for i, line in enumerate(lines) if not line.startswith('#'))\n",
    "\n",
    "        # Read data starting at detected header\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep='\\t',\n",
    "            comment='#',\n",
    "            header=0,\n",
    "            dtype=str,\n",
    "            engine='python'\n",
    "        )\n",
    "\n",
    "        # Clean and convert key columns\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df['peak_dt'] = pd.to_datetime(df['peak_dt'], errors='coerce')\n",
    "        df['peak_va'] = pd.to_numeric(df['peak_va'], errors='coerce')\n",
    "\n",
    "        # Drop rows with invalid data\n",
    "        df_clean = df[['site_no', 'peak_dt', 'peak_va']].dropna()\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# üîç Example usage\n",
    "# --- Step 1: Load USGS Peak Flow Data (Annual Max Series) ---\n",
    "# Replace this with the path to your downloaded USGS data (CSV or TXT)\n",
    "\n",
    "usgs_file = \"07022500_nwis_peak.txt\"\n",
    "peak_df = read_nwis_peak_file(usgs_file)\n",
    "\n",
    "# üìä Preview the data\n",
    "print(peak_df.head())\n",
    "\n",
    "# üìö Distributions to evaluate\n",
    "distributions = {\n",
    "    \"Gumbel (EV1)\": stats.gumbel_r,\n",
    "    \"Log-Pearson III\": stats.pearson3,\n",
    "    \"GEV\": stats.genextreme,\n",
    "    \"Normal\": stats.norm,\n",
    "    \"Lognormal\": stats.lognorm,\n",
    "    \"Weibull\": stats.weibull_min,\n",
    "    \"Exponential\": stats.expon,\n",
    "    \"Gamma\": stats.gamma,\n",
    "    \"Loglogistic\": stats.fisk,\n",
    "    \"Generalized Pareto\": stats.genpareto\n",
    "}\n",
    "\n",
    "# üß† Evaluate and store results\n",
    "summary_rows = []\n",
    "fit_results = {}\n",
    "\n",
    "for name, dist in distributions.items():\n",
    "    try:\n",
    "        params = dist.fit(flow_data)\n",
    "        flood_q = dist.ppf(1 - prob_exceed, *params)\n",
    "        q_estimates = dist.ppf(prob_plot, *params)\n",
    "        rmse = np.sqrt(mean_squared_error(sorted_data, q_estimates))\n",
    "        ks_stat, ks_pval = kstest(flow_data, dist.cdf, args=params)\n",
    "\n",
    "        fit_results[name] = {\n",
    "            \"params\": params,\n",
    "            \"q\": flood_q,\n",
    "            \"rmse\": rmse,\n",
    "            \"ks_stat\": ks_stat,\n",
    "            \"ks_pval\": ks_pval\n",
    "        }\n",
    "\n",
    "        param_str = \", \".join([f\"{p:.2f}\" for p in params])\n",
    "        summary_rows.append({\n",
    "            \"Distribution\": name,\n",
    "            \"Parameters\": param_str,\n",
    "            \"RMSE (cfs)\": round(rmse, 2),\n",
    "            \"KS Stat\": round(ks_stat, 3),\n",
    "            \"KS p-value\": round(ks_pval, 3),\n",
    "            \"KS Result\": \"‚úÖ Pass\" if ks_pval > 0.05 else \"‚ùå Reject\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not fit {name}: {e}\")\n",
    "\n",
    "# üìä Display Summary Table\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(by=\"RMSE (cfs)\")\n",
    "print(\"\\nüìä Goodness-of-Fit Summary for All Distributions:\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# üìà Interactive Plotting\n",
    "def plot_selected_distribution(dist_name):\n",
    "    result = fit_results[dist_name]\n",
    "    q = result[\"q\"]\n",
    "    params = result[\"params\"]\n",
    "    param_str = \", \".join([f\"{p:.2f}\" for p in params])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(return_periods, q, marker='o', linestyle='-', color='royalblue', label=\"Estimated Peak Flow\")\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Return Period (years, log scale)\")\n",
    "    plt.ylabel(\"Estimated Peak Flow (cfs)\")\n",
    "    plt.title(f\"{dist_name} Flood Frequency Curve\\nParameters: {param_str}\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show tabular result\n",
    "    df_plot = pd.DataFrame({\n",
    "        \"Return Period (yr)\": return_periods,\n",
    "        \"Estimated Peak Flow (cfs)\": q.round(2)\n",
    "    })\n",
    "    print(f\"\\nüìå Parameters: {param_str}\")\n",
    "    print(f\"RMSE: {result['rmse']:.2f}, KS stat: {result['ks_stat']:.3f}, p-value: {result['ks_pval']:.3f}\")\n",
    "    display(df_plot)\n",
    "\n",
    "# üîÄ Launch dropdown selector\n",
    "interact(plot_selected_distribution, dist_name=Dropdown(options=list(fit_results.keys()), description=\"Distribution\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47ac05-66f3-4001-9f83-66350098ecbf",
   "metadata": {},
   "source": [
    "# üìò Self-Assessment: Flood Frequency Analysis Tool\n",
    "\n",
    "Use these prompts and questions to evaluate your understanding of the tool and its underlying hydrologic and statistical concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Conceptual Questions\n",
    "\n",
    "1. **Why are return periods plotted on a logarithmic scale in flood frequency analysis?**\n",
    "   - *Hint: Think about how frequent vs. rare events are distributed.*\n",
    "\n",
    "2. **What is the purpose of fitting multiple distributions to the same peak flow dataset?**\n",
    "   - *Hint: No single distribution fits all scenarios equally well.*\n",
    "\n",
    "3. **How do Gringorten plotting positions help in flood frequency analysis?**\n",
    "   - *Hint: They're used to assign empirical probabilities to ordered data.*\n",
    "\n",
    "4. **What assumptions underlie the use of the Gumbel distribution in hydrology?**\n",
    "   - *Hint: It‚Äôs designed to model block maxima like annual peak flows.*\n",
    "\n",
    "5. **How do parametric and non-parametric flood frequency methods differ in their approach?**\n",
    "   - *Hint: Consider how the data distribution is treated.*\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Reflective Prompts\n",
    "\n",
    "1. **If two distributions yield similar RMSE but different KS p-values, which metric is more important for selecting a model‚Äîand why?**\n",
    "\n",
    "2. **Can a statistically good-fitting distribution be inappropriate for design applications? Provide an example.**\n",
    "\n",
    "3. **How would you adapt this tool to process data from multiple gage stations simultaneously?**\n",
    "\n",
    "4. **What limitations might this tool face when applied to future climate-affected streamflow patterns?**\n",
    "\n",
    "5. **How would the analysis change if you used partial-duration series instead of annual maxima?**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Quiz Questions\n",
    "\n",
    "**Q1.** The Gumbel distribution is commonly used to model:  \n",
    "A. Rainfall intensity  \n",
    "B. Annual maximum values  \n",
    "C. Median flow durations  \n",
    "D. Baseflow during drought  \n",
    "‚úÖ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q2.** The Kolmogorov‚ÄìSmirnov test compares:  \n",
    "A. Log and normal distributions  \n",
    "B. ECDF and theoretical CDF  \n",
    "C. Mean annual rainfall  \n",
    "D. Number of peaks above threshold  \n",
    "‚úÖ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q3.** In the Generalized Extreme Value distribution, the shape parameter controls:  \n",
    "A. Peak discharge  \n",
    "B. Tail behavior  \n",
    "C. Cumulative runoff  \n",
    "D. Frequency of low flows  \n",
    "‚úÖ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q4.** A high KS p-value and low RMSE suggest:  \n",
    "A. Overfitting  \n",
    "B. Good model fit  \n",
    "C. Poor data resolution  \n",
    "D. Statistical bias  \n",
    "‚úÖ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q5.** Which distribution is least appropriate for positively skewed hydrologic data?  \n",
    "A. Gumbel  \n",
    "B. Lognormal  \n",
    "C. Normal  \n",
    "D. Log-Pearson III  \n",
    "‚úÖ **Correct:** C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d100d89-68e3-4c92-9cb3-a8f04eec3a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Passed header=[72], len of 1, but only 36 lines in file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:407\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_pos \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m hr:\n\u001b[1;32m--> 407\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_line()\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:738\u001b[0m, in \u001b[0;36mPythonParser._next_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 738\u001b[0m     orig_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_iter_line(row_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:805\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[1;34m(self, row_num)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 805\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# üîΩ Input file path\u001b[39;00m\n\u001b[0;32m     22\u001b[0m usgs_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m07022500_nwis_peak.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual file path\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m peak_df \u001b[38;5;241m=\u001b[39m read_nwis_peak_file(usgs_file)\n\u001b[0;32m     24\u001b[0m flow_data \u001b[38;5;241m=\u001b[39m peak_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_va\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# üåä Return periods and exceedance probabilities\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m, in \u001b[0;36mread_nwis_peak_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m     14\u001b[0m header_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(i \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_dt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m line \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39mheader_line, comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_va\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_va\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:133\u001b[0m, in \u001b[0;36mPythonParser.__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    128\u001b[0m columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]]\n\u001b[0;32m    129\u001b[0m (\n\u001b[0;32m    130\u001b[0m     columns,\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_original_columns,\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols,\n\u001b[1;32m--> 133\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_columns()\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[0;32m    138\u001b[0m (\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    146\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:417\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m     joi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, header[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m have_mi_columns \u001b[38;5;28;01melse\u001b[39;00m header))\n\u001b[0;32m    416\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(joi)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], len of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(joi)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed header=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lines in file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# We have an empty file, so check\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# if columns are provided. That will\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# serve as the 'line' for parsing\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_mi_columns \u001b[38;5;129;01mand\u001b[39;00m hr \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Passed header=[72], len of 1, but only 36 lines in file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import kstest\n",
    "from ipywidgets import interact, Dropdown\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Load USGS NWIS peak flow data ---\n",
    "def read_nwis_peak_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    header_line = next(i for i, line in enumerate(lines) if 'peak_dt' in line and not line.startswith('#'))\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=header_line, comment='#', engine='python')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df['peak_va'] = pd.to_numeric(df['peak_va'], errors='coerce')\n",
    "    df = df.dropna(subset=['peak_va'])\n",
    "    return df\n",
    "\n",
    "# üîΩ Input file path\n",
    "usgs_file = \"07022500_nwis_peak.txt\"  # Replace with the actual file path\n",
    "peak_df = read_nwis_peak_file(usgs_file)\n",
    "flow_data = peak_df['peak_va'].values\n",
    "\n",
    "# üåä Return periods and exceedance probabilities\n",
    "return_periods = np.array([2, 5, 10, 25, 50, 100])\n",
    "prob_exceed = 1 / return_periods\n",
    "sorted_data = np.sort(flow_data)\n",
    "n = len(sorted_data)\n",
    "prob_plot = (np.arange(1, n + 1) - 0.44) / (n + 0.12)  # Gringorten plotting positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e0a0a-6d4a-4231-aad1-3981454673f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
